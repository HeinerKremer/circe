experiment:
  description: regression w/ VMM
  output_location: results/yaleb_cone/
  load: null
  resume: false

data:
  data_key: yale-b

  train: data/ExtendedYaleB/train
  val: data/ExtendedYaleB/val
  val_ood: data/ExtendedYaleB/val
  test: data/ExtendedYaleB/test
  test_ood: data/ExtendedYaleB/test
  nl_type: y-cone
  noise: 2
  holdout_ratio: 0.1
  batch_size: 256
  num_workers: 16

model:
  model_key: regressor
  trainer_key: vmm
  modes:
    - train
    - val
    - val_ood
    - test
    - test_ood
  epochs: 200
  patience: 50

  lr_setup: 7
  trainer_config:
    reg_param: 0
    theta_reg_param: 0
    dual_func_network_kwargs:
      layer_widths:
        - 50
        - 20
      activation: LeakyReLU
      normalized: False
    theta_optim_args:
      optimizer:
        AdamW:
          lr: 0.01
          weight_decay: 0.01
      scheduler:
        CosineAnnealingLR:
          T_max: 200
    dual_optim_args:
      optimizer:
        OAdam:
          lr: 0.02
          betas:
            - 0.5
            - 0.9
      scheduler:
        CosineAnnealingLR:
          T_max: 200

    # Fixed arguments
    default: False
    gpu: True
    gradient_descent_ascent: True
    pretrain: False
    pretrain_dual: False
    progress_bar: True

    # Following arguments are unused as batching/training is done in the wrapper class
    max_num_epochs: False
    batch_size: False
    eval_freq: False
    max_no_improve: False
    burn_in_cycles: False
    early_stopping: False

    # Unused arguments
    divergence: chi2
    reg_param_rkhs_norm: 0.0
    val_loss_func: hsic

  network:
    featurizer: resnet18

    fc1:
      - Linear:
          in_features: 512
          out_features: 128
      - LeakyReLU:
          inplace: True

    fc2:
      - Linear:
          in_features: 128
          out_features: 64

    target:
      - Linear:
          in_features: 64
          out_features: 1